# Wikipedia 中文数据清洗工具

一个专为大模型预训练优化的维基百科中文数据清洗工具，能够从Wikipedia dump文件中提取高质量的中文训练语料。

## 🚀 快速开始

### 环境要求

- Python 3.6+
- 依赖库：
  ```bash
  pip install mwxml
  ```

**说明**：
- `mwxml`：用于解析维基百科XML dump文件
- 其他库均为Python标准库：`bz2`, `re`, `json`, `argparse`, `sys`, `pathlib`, `html.entities`

### 获取数据

从维基百科官方下载中文dump文件：
```bash
# 完整dump文件 (约3.1GB)
wget https://dumps.wikimedia.org/zhwiki/20250601/zhwiki-20250601-pages-articles-multistream.xml.bz2

# 或者下载分块文件进行测试 (约231MB)
wget https://dumps.wikimedia.org/zhwiki/20250601/zhwiki-20250601-pages-articles-multistream1.xml-p1p187712.bz2
```

### 基本用法

```bash
# 处理测试分块文件
python clean.py zhwiki-20250601-pages-articles-multistream1.xml-p1p187712.bz2

# 处理完整dump文件
python clean.py zhwiki-20250601-pages-articles-multistream.xml.bz2

# 快速测试：只处理1000篇文章
python clean.py dump.xml.bz2 --max-articles 2000

# 自定义输出路径和样例数量
python clean.py dump.xml.bz2 --output clean.jsonl --sample sample.jsonl --sample-size 1000

# 完整参数示例
python clean.py dump.xml.bz2 --output clean.jsonl --sample sample.jsonl --sample-size 500 --max-articles 2000

# 查看帮助
python clean.py --help
```

### 参数说明

- `dump_path`: 维基百科dump文件路径 (*.xml.bz2)
- `--output`: 输出JSONL文件路径 (默认: zhwiki_cleaned.jsonl)
- `--sample`: 样例文件路径 (默认: sample_1000.jsonl)
- `--sample-size`: 样例数量 (默认: 1000)
- `--max-articles`: 最大处理文章数 (可选，用于快速测试)

## 🔧 开发过程中遇到的问题和解决方案


### 1. 清洗策略不够彻底

**问题**: 
- 简单的正则表达式无法处理复杂的嵌套结构
- 中文特有的格式没有充分考虑
- 质量过滤标准不够严格

**解决方案**:
- 实现了7阶段渐进式清洗策略
- 添加中文特化的过滤规则

```python
def dropNested(text, openDelim, closeDelim):
    """智能处理嵌套结构，如 {{模板{{嵌套}}}} """
    # 使用栈结构正确匹配嵌套的开始和结束标记
```

### 2. 性能和内存问题

**问题**: 
- 大文件处理时内存占用过高
- 处理速度慢，缺乏进度反馈

**解决方案**:
- 采用流式处理，避免一次性加载整个文件
- 定期flush输出缓冲区
- 添加详细的进度报告

```python
# 每1000页输出进度
if processed_pages % 1000 == 0:
    print(f"已处理页面: {processed_pages:,}, 有效文章: {valid_articles:,}")
    out_f.flush()
```

### 3. 输出格式和质量控制

**问题**: 
- 输出的文本质量参差不齐
- 缺乏有效的质量指标
- 没有便于检查的样例文件

**解决方案**:
- 实施严格的质量过滤标准
- 同时生成完整输出和样例文件
- 在meta信息中包含文本长度等指标

```python
# 质量检查标准
if (len(text) < 100 or 
    len(re.findall(r'[\u4e00-\u9fff]', text)) < len(text) * 0.5 or
    len(re.findall(r'[\u4e00-\u9fff]', text)) < 50):
    return ""
```

### 4. 错误处理和用户体验

**问题**: 
- 缺乏错误处理机制
- 用户不知道如何正确使用工具
- 没有版本信息和帮助文档

**解决方案**:
- 添加完整的异常处理
- 提供详细的帮助信息和使用示例
- 检查文件存在性和依赖库

```python
try:
    from mwxml import Dump
except ImportError:
    print("错误: 需要安装 mwxml 库")
    print("请运行: pip install mwxml")
    sys.exit(1)
```

## 📋 清洗策略

本工具采用7阶段渐进式清洗策略，专门针对中文维基百科的特点进行优化：

### 第一阶段：结构性清理

**目标**: 移除大块的无用内容，减少后续处理负担

- **早期截断**: 识别并移除"参考文献"、"外部链接"等章节后的所有内容
- **HTML注释清理**: 移除 `<!--...-->` 注释
- **模板清理**: 移除维基模板 `{{...}}`，这些通常包含导航、信息框等非正文内容
- **表格清理**: 移除表格结构 `{|...|}` 

```python
# 示例：移除参考文献后的内容
end_sections = r'(参见|注释|参考文献?|参考书目|外部链接|延伸阅读|相关条目|另见|参考资料|脚注)'
```

### 第二阶段：链接处理

**目标**: 智能处理维基百科的内部和外部链接

- **维基链接处理**: 
  - 保留链接的显示文本: `[[目标|显示文本]]` → `显示文本`
  - 移除文件/图片/分类链接: `[[File:...]]` → (删除)
  - 处理链接尾缀: `[[链接]]s` → `链接s`

- **外部链接处理**:
  - 保留链接文本: `[URL 文本]` → `文本`
  - 移除裸URL和纯链接

### 第三阶段：HTML清理

**目标**: 彻底清除HTML标记，保留纯文本

- **丢弃元素**: 完全移除 `<table>`, `<ref>`, `<gallery>` 等及其内容
- **忽略标签**: 移除 `<b>`, `<i>`, `<div>` 等标签但保留内容
- **自闭合标签**: 移除 `<br/>`, `<hr/>` 等

### 第四阶段：格式清理

**目标**: 处理维基百科特有的格式标记

- **粗体/斜体**: `'''粗体'''` → `粗体`, `''斜体''` → `斜体`
- **HTML实体解码**: `&nbsp;` → 空格, `&amp;` → `&`
- **二次解码**: 处理嵌套编码的实体

### 第五阶段：中文特化清理

**目标**: 针对中文维基百科的特殊内容进行清理

- **多语言标记**: 移除 `-zh-hans:简体-` 等语言变体标记
- **引用标记**: 移除 `[1]`, `[2]` 等参考文献编号
- **学术标识**: 移除 ISBN, DOI 等学术出版信息
- **图片描述词**: 移除 "缩略图", "右", "300px" 等图片描述

### 第六阶段：智能过滤

**目标**: 基于内容质量进行智能筛选

**按行过滤标准**:
- 章节标题 (`=标题=`) → 删除
- 过短文本 (< 15字符) → 删除  
- 中文比例过低 (< 30%) → 删除
- 明显的图片描述 (多逗号但无句号) → 删除
- 列表项 (`*`, `#` 开头) → 删除

**整体质量检查**:
- 最小长度: 100字符
- 中文比例: ≥ 50%
- 最少中文字符: 50个

### 第七阶段：最终清理

**目标**: 规范化文本格式，提升可读性

- **标点规范化**: 统一中文标点的spacing
- **符号清理**: 移除残留的括号、竖线等
- **重复标点**: `。。` → `。`
- **空白处理**: 规范化空格和换行

## 📊 输出格式

每行为一个JSON对象：

```json
{
  "text": "清洗后的文本内容...",
  "meta": {
    "title": "文章标题",
    "id": 12345,
    "length": 1500,
    "chinese_ratio": 0.856
  }
}
```

## 📈 效果评估

以下是处理 `zhwiki-20250601-pages-articles-multistream1.xml-p1p187712.bz2` (231MB) 的实际效果：

```
开始处理: zhwiki-20250601-pages-articles-multistream1.xml-p1p187712.bz2
输出文件: zhwiki_cleaned.jsonl        
样例文件: sample_1000.jsonl (前1000条)
最大处理文章数: 2000
--------------------------------------------------
已处理页面: 1,000, 有效文章: 657, 样例: 657, 速度: 6457页/分钟
已处理页面: 2,000, 有效文章: 1,258, 样例: 1000, 速度: 7560页/分钟
已处理页面: 3,000, 有效文章: 1,810, 样例: 1000, 速度: 8669页/分钟

已达到最大文章数限制: 2000

============================================================
📊 处理完成 - 效果评估报告
============================================================
📈 基本统计:
  总页面数: 3,482
  有效文章: 2,000
  样例数量: 1000
  文章数限制: 2,000 (已达到)
  过滤比例: 42.6%
  处理时间: 22.4秒 (0.4分钟)
  处理速度: 9327页/分钟

📋 质量指标:
  平均文本长度: 3652字符
  平均中文比例: 80.4%
  总字符数: 7,303,048
  总中文字符: 5,923,046

📊 长度分布:
  短文本 (<500字符): 352 (17.6%)
  中等文本 (500-2000字符): 699 (34.9%)
  长文本 (>2000字符): 949 (47.4%)

🈳 中文比例分布:
  高中文比例 (≥80%): 1,288 (64.4%)
  中等中文比例 (50-80%): 712 (35.6%)
  低中文比例 (<50%): 0 (0.0%)
============================================================
```


## 🎯 使用建议

### 文件选择
- **测试**: 使用231MB的分块文件快速验证效果
- **生产**: 使用3.1GB的完整dump文件获得全部数据
- **更新**: 定期下载最新的dump文件保持数据新鲜度

### 快速测试
```bash
# 只处理1000篇文章进行快速测试
python clean.py dump.xml.bz2 --max-articles 2000

# 处理更多文章进行详细评估
python clean.py dump.xml.bz2 --max-articles 5000
```

### 质量验证
1. 检查样例文件确认清洗效果
2. 查看处理完成后的详细统计报告
3. 人工抽查部分文本质量

### 批量处理
```bash
# 处理多个dump文件
for file in *.xml.bz2; do
    python clean.py "$file" --output "${file%.xml.bz2}_cleaned.jsonl"
done
```

## 🚨 常见问题

### Q: 提示"需要安装 mwxml 库"怎么办？
A: 运行 `pip install mwxml` 安装依赖库

### Q: 处理速度很慢怎么办？
A: 
- 先用小文件测试确认效果
- 使用 `--max-articles 2000` 进行快速测试
- 大文件处理需要耐心等待，可以通过进度输出监控
- 考虑使用SSD硬盘提高I/O速度

### Q: 输出的文本质量不满意怎么办？
A: 
- 检查样例文件，了解当前清洗效果
- 查看处理完成后的详细评估报告
- 可以调整 `clean_text()` 函数中的过滤参数
- 根据具体需求修改质量检查标准

### Q: 如何处理其他语言的维基百科？
A: 
- 修改 `end_sections` 正则表达式适配其他语言
- 调整中文字符检查逻辑
- 根据目标语言特点调整清洗策略

### Q: 为什么我的测试结果和示例不同？
A:
- 不同的dump文件内容质量可能不同
- 处理的文章数量会影响统计结果
- 硬件性能差异会影响处理速度
- 可以通过调整参数来优化结果

## 📝 License

MIT License

## 🤝 贡献

欢迎提交Issues和Pull Requests来改进这个工具！

特别欢迎以下类型的贡献：
- 新的清洗策略和过滤规则
- 性能优化建议
- 其他语言的适配
- 文档改进
- 测试结果和使用反馈

---

*专为中文大模型预训练设计，确保高质量的训练语料。*
