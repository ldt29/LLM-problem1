#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Wikipedia ä¸­æ–‡æ•°æ®æ¸…æ´—å·¥å…· - ä¸“ä¸ºå¤§æ¨¡å‹é¢„è®­ç»ƒä¼˜åŒ–

ç”¨æ³•:
    python clean.py [dump_file] --output output.jsonl --sample sample.jsonl
"""

import bz2
import re
import json
import argparse
import sys
from pathlib import Path
from html.entities import name2codepoint

# å…¨å±€é…ç½®
acceptedNamespaces = set(['w'])  # åªæ¥å—ä¸»å‘½åç©ºé—´

# éœ€è¦ä¸¢å¼ƒçš„HTMLå…ƒç´ 
discardElements = set([
    'gallery', 'timeline', 'noinclude', 'pre', 'table', 'tr', 'td', 'th', 'caption',
    'form', 'input', 'select', 'option', 'textarea', 'ul', 'li', 'ol', 'dl', 'dt', 'dd',
    'menu', 'dir', 'ref', 'references', 'img', 'imagemap', 'source', 'math', 'code'
])

# å¿½ç•¥çš„æ ‡ç­¾ï¼ˆä¿ç•™å†…å®¹ï¼Œå»æ‰æ ‡ç­¾ï¼‰
ignoredTags = [
    'b', 'big', 'blockquote', 'center', 'cite', 'div', 'em', 'font', 
    'h1', 'h2', 'h3', 'h4', 'hiero', 'i', 'kbd', 'nowiki', 'p', 
    'plaintext', 's', 'small', 'span', 'strike', 'strong', 'sub', 'sup', 'tt', 'u', 'var'
]

# è‡ªé—­åˆæ ‡ç­¾
selfClosingTags = ['br', 'hr', 'nobr', 'ref', 'references']

def unescape(text):
    """ç§»é™¤HTMLå­—ç¬¦å¼•ç”¨å’Œå®ä½“"""
    def fixup(m):
        text = m.group(0)
        code = m.group(1)
        try:
            if text[1] == "#":  # å­—ç¬¦å¼•ç”¨
                if text[2] == "x":
                    return chr(int(code[1:], 16))
                else:
                    return chr(int(code))
            else:  # å‘½åå®ä½“
                return chr(name2codepoint[code])
        except:
            return text  # ä¿æŒåŸæ ·
    return re.sub("&#?(\w+);", fixup, text)

def dropNested(text, openDelim, closeDelim):
    """ç§»é™¤åµŒå¥—çš„ç»“æ„ï¼Œå¦‚æ¨¡æ¿{{}}å’Œè¡¨æ ¼{||}"""
    openRE = re.compile(openDelim)
    closeRE = re.compile(closeDelim)
    matches = []
    nest = 0
    start = openRE.search(text, 0)
    if not start:
        return text
    end = closeRE.search(text, start.end())
    next = start
    while end:
        next = openRE.search(text, next.end())
        if not next:
            while nest:
                nest -= 1
                end0 = closeRE.search(text, end.end())
                if end0:
                    end = end0
                else:
                    break
            matches.append((start.start(), end.end()))
            break
        while end.end() < next.start():
            if nest:
                nest -= 1
                last = end.end()
                end = closeRE.search(text, end.end())
                if not end:
                    if matches:
                        span = (matches[0][0], last)
                    else:
                        span = (start.start(), last)
                    matches = [span]
                    break
            else:
                matches.append((start.start(), end.end()))
                start = next
                end = closeRE.search(text, next.end())
                break
        if next != start:
            nest += 1
    
    # æ”¶é›†åŒ¹é…åŒºåŸŸå¤–çš„æ–‡æœ¬
    res = ''
    start = 0
    for s, e in matches:
        res += text[start:s]
        start = e
    res += text[start:]
    return res

def clean_text(text: str) -> str:
    """
    å½»åº•æ¸…æ´—ç»´åŸºç™¾ç§‘æ–‡æœ¬ï¼Œä¸“ä¸ºå¤§æ¨¡å‹é¢„è®­ç»ƒä¼˜åŒ–
    """
    
    # ========== ç¬¬ä¸€é˜¶æ®µï¼šç»“æ„æ€§æ¸…ç† ==========
    
    # 1. æ—©æœŸæˆªæ–­ï¼šç§»é™¤å‚è€ƒæ–‡çŒ®ç­‰ç« èŠ‚åçš„æ‰€æœ‰å†…å®¹
    end_sections = r'(å‚è§|æ³¨é‡Š|å‚è€ƒæ–‡çŒ®?|å‚è€ƒä¹¦ç›®|å¤–éƒ¨é“¾æ¥|å»¶ä¼¸é˜…è¯»|ç›¸å…³æ¡ç›®|å¦è§|å‚è€ƒèµ„æ–™|è„šæ³¨)'
    match = re.search(end_sections, text, re.IGNORECASE)
    if match:
        text = text[:match.start()]
    
    # 2. ç§»é™¤HTMLæ³¨é‡Š
    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)
    
    text = dropNested(text, r'{{', r'}}')  # æ¨¡æ¿
    text = dropNested(text, r'{\|', r'\|}')  # è¡¨æ ¼
    
    # ========== ç¬¬äºŒé˜¶æ®µï¼šé“¾æ¥å¤„ç† ==========
    
    # 4. å¤„ç†ç»´åŸºé“¾æ¥ [[...]]
    # å…ˆå¤„ç†å¸¦æ˜¾ç¤ºæ–‡æœ¬çš„é“¾æ¥ [[ç›®æ ‡|æ˜¾ç¤ºæ–‡æœ¬]]
    def process_wikilink(match):
        link = match.group(1)
        anchor = match.group(2) if match.group(2) else link
        trail = match.group(3) if match.group(3) else ''
        
        # æ£€æŸ¥å‘½åç©ºé—´
        colon = link.find(':')
        if colon > 0:
            namespace = link[:colon].lower()
            if namespace in ['file', 'image', 'media', 'category']:
                return ''  # å®Œå…¨ç§»é™¤æ–‡ä»¶/åˆ†ç±»é“¾æ¥
        
        return anchor + trail
    
    # åŒ¹é…ç»´åŸºé“¾æ¥
    wikiLink = re.compile(r'\[\[([^[]*?)(?:\|([^[]*?))?\]\](\w*)')
    text = wikiLink.sub(process_wikilink, text)
    
    # ç§»é™¤å‰©ä½™çš„å‚æ•°åŒ–é“¾æ¥
    text = re.sub(r'\[\[.*?\]\]', '', text)
    
    # 5. å¤„ç†å¤–éƒ¨é“¾æ¥
    text = re.sub(r'\[https?://[^\s\]]+\s+([^\]]+)\]', r'\1', text)  # [URL æ–‡æœ¬] â†’ æ–‡æœ¬
    text = re.sub(r'\[https?://[^\]]+\]', '', text)  # [URL] â†’ åˆ é™¤
    text = re.sub(r'https?://[^\s\n]+', '', text)  # è£¸URL
    
    # ========== ç¬¬ä¸‰é˜¶æ®µï¼šHTMLæ¸…ç† ==========
    
    # 6. ç§»é™¤ä¸¢å¼ƒçš„å…ƒç´ 
    for tag in discardElements:
        pattern = re.compile(r'<\s*%s\b[^>]*>.*?<\s*/\s*%s>' % (tag, tag), re.DOTALL | re.IGNORECASE)
        text = pattern.sub('', text)
    
    # 7. ç§»é™¤è‡ªé—­åˆæ ‡ç­¾
    for tag in selfClosingTags:
        pattern = re.compile(r'<\s*%s\b[^/]*/\s*>' % tag, re.DOTALL | re.IGNORECASE)
        text = pattern.sub('', text)
    
    # 8. ç§»é™¤å¿½ç•¥çš„æ ‡ç­¾ï¼ˆä¿ç•™å†…å®¹ï¼‰
    for tag in ignoredTags:
        text = re.sub(r'<\s*%s\b[^>]*>' % tag, '', text, flags=re.IGNORECASE)
        text = re.sub(r'<\s*/\s*%s>' % tag, '', text, flags=re.IGNORECASE)
    
    # 9. ç§»é™¤å‰©ä½™çš„HTMLæ ‡ç­¾
    text = re.sub(r'<[^>]*>', '', text)
    
    # ========== ç¬¬å››é˜¶æ®µï¼šæ ¼å¼æ¸…ç† ==========
    
    # 10. å¤„ç†ç²—ä½“/æ–œä½“
    text = re.sub(r"'''''([^']*?)'''''", r'\1', text)  # ç²—æ–œä½“
    text = re.sub(r"'''(.*?)'''", r'\1', text)  # ç²—ä½“
    text = re.sub(r"''([^']*)''", r'\1', text)  # æ–œä½“
    
    # 11. HTMLå®ä½“è§£ç 
    text = unescape(text)
    text = unescape(text)  # äºŒæ¬¡è§£ç å¤„ç† &amp;nbsp; ç­‰
    
    # ========== ç¬¬äº”é˜¶æ®µï¼šä¸­æ–‡ç‰¹åŒ–æ¸…ç† ==========
    
    # 12. ç§»é™¤å¤šè¯­è¨€æ ‡è®°
    text = re.sub(r'-zh-[^:]*:[^-]*-', '', text)
    text = re.sub(r'-\{[^}]*\}-', '', text)
    
    # 13. ç§»é™¤å¼•ç”¨æ ‡è®°å’Œå­¦æœ¯æ ‡è¯†
    text = re.sub(r'\[\d+\]', '', text)  # [1], [2] ç­‰
    text = re.sub(r'ISBN\s*[\d\-X]+', '', text)
    text = re.sub(r'DOI\s*:?\s*[\d\./]+', '', text)
    text = re.sub(r'\d{4}\s*reprint\.?', '', text)
    
    # 14. ç§»é™¤å›¾ç‰‡æè¿°è¯æ±‡
    image_words = r'\b(thumb|thumbnail|right|left|center|frame|frameless|border|upright|\d+px|ç¼©ç•¥å›¾|å³|å·¦|å±…ä¸­|æ¡†æ¶)\b'
    text = re.sub(image_words, '', text, flags=re.IGNORECASE)
    
    # 15. ç§»é™¤è¡Œå†…å…¬å¼å’Œèµ‹å€¼
    text = re.sub(r'\([^\)]*=[^\)]*\)', '', text)  # (x = y)
    text = re.sub(r'\b[\w\-\+\*/^]{1,20}\s*=\s*[\w\-\+\*/^]{1,20}\b', '', text)  # a=b
    
    # ========== ç¬¬å…­é˜¶æ®µï¼šæ™ºèƒ½è¿‡æ»¤ ==========
    
    # 16. æŒ‰è¡Œè¿‡æ»¤
    lines = text.split('\n')
    cleaned_lines = []
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
        
        # è·³è¿‡ç« èŠ‚æ ‡é¢˜æ ¼å¼
        if re.match(r'=+.*?=+', line):
            continue
            
        # è·³è¿‡æ˜æ˜¾çš„å›¾ç‰‡æè¿°
        if (len(line) < 200 and 
            line.count('ï¼Œ') > 2 and
            not any(char in line for char in ['ã€‚', 'ï¼', 'ï¼Ÿ'])):
            continue
        
        # è·³è¿‡è¿‡çŸ­çš„è¡Œ
        if len(line) < 15:
            continue
        
        # è·³è¿‡éä¸­æ–‡ä¸ºä¸»çš„è¡Œ
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', line))
        if chinese_chars < len(line) * 0.3:
            continue
        
        # è·³è¿‡åˆ—è¡¨é¡¹
        if re.match(r'^\s*[*#:;â€¢Â·]', line):
            continue
            
        cleaned_lines.append(line)
    
    # 17. é‡æ–°ç»„åˆæ–‡æœ¬
    text = ' '.join(cleaned_lines)
    
    # ========== ç¬¬ä¸ƒé˜¶æ®µï¼šæœ€ç»ˆæ¸…ç† ==========
    
    # 18. æ ‡ç‚¹ç¬¦å·è§„èŒƒåŒ–
    text = re.sub(r'\s*([ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š])\s*', r'\1', text)
    text = re.sub(r'([ã€‚ï¼ï¼Ÿ])\s*([^\s])', r'\1 \2', text)
    text = re.sub(r'\s+', ' ', text)
    
    # 19. ç§»é™¤æ®‹ç•™ç¬¦å·
    text = re.sub(r'[|{}()ï¼ˆï¼‰\[\]<>]', '', text)
    text = re.sub(r'[ï¼Œã€‚]{2,}', 'ã€‚', text)
    text = re.sub(r'^\s*[ã€‚ï¼Œï¼ï¼Ÿï¼›ï¼š]\s*', '', text)  # å¼€å¤´çš„æ ‡ç‚¹
    
    # 20. æœ€ç»ˆè´¨é‡æ£€æŸ¥
    text = text.strip()
    
    # å¦‚æœæ–‡æœ¬å¤ªçŸ­æˆ–ä¸­æ–‡æ¯”ä¾‹å¤ªä½ï¼Œè¿”å›ç©º
    if (len(text) < 100 or 
        len(re.findall(r'[\u4e00-\u9fff]', text)) < len(text) * 0.5 or
        len(re.findall(r'[\u4e00-\u9fff]', text)) < 50):
        return ""
    
    return text

def process_dump(dump_path: str, output_path: str, sample_path: str, sample_size=1000, max_articles=None):
    """å¤„ç†ç»´åŸºç™¾ç§‘dumpæ–‡ä»¶"""
    try:
        from mwxml import Dump
    except ImportError:
        print("é”™è¯¯: éœ€è¦å®‰è£… mwxml åº“")
        print("è¯·è¿è¡Œ: pip install mwxml")
        sys.exit(1)
    
    import time
    start_time = time.time()
    
    # ç»Ÿè®¡å˜é‡
    sample_count = 0
    processed_pages = 0
    valid_articles = 0
    total_text_length = 0
    total_chinese_chars = 0
    text_lengths = []  # ç”¨äºè®¡ç®—å¹³å‡é•¿åº¦
    chinese_ratios = []  # ç”¨äºè®¡ç®—ä¸­æ–‡æ¯”ä¾‹
    
    print(f"å¼€å§‹å¤„ç†: {dump_path}")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_path}")
    print(f"æ ·ä¾‹æ–‡ä»¶: {sample_path} (å‰{sample_size}æ¡)")
    if max_articles:
        print(f"æœ€å¤§å¤„ç†æ–‡ç« æ•°: {max_articles}")
    print("-" * 50)
    
    with bz2.open(dump_path, 'rb') as f, \
         open(output_path, 'w', encoding='utf-8') as out_f, \
         open(sample_path, 'w', encoding='utf-8') as sample_f:
        
        dump = Dump.from_file(f)
        
        for page in dump:
            processed_pages += 1
            
            # è·³è¿‡éä¸»å‘½åç©ºé—´æˆ–é‡å®šå‘é¡µ
            if page.namespace != 0 or page.redirect:
                continue
            
            # å¤„ç†é¡µé¢çš„æœ€æ–°ç‰ˆæœ¬
            for revision in page:
                text = revision.text or ""
                cleaned = clean_text(text)
                
                # è·³è¿‡æ¸…æ´—åä¸ºç©ºæˆ–è¿‡çŸ­çš„æ–‡æœ¬
                if not cleaned or len(cleaned) < 100:
                    continue
                
                # ç»Ÿè®¡æ–‡æœ¬è´¨é‡æŒ‡æ ‡
                text_length = len(cleaned)
                chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', cleaned))
                chinese_ratio = chinese_chars / text_length if text_length > 0 else 0
                
                total_text_length += text_length
                total_chinese_chars += chinese_chars
                text_lengths.append(text_length)
                chinese_ratios.append(chinese_ratio)
                
                # æ„å»ºJSONå¯¹è±¡
                obj = {
                    "text": cleaned,
                    "meta": {
                        "title": page.title,
                        "id": page.id,
                        "length": text_length,
                        "chinese_ratio": round(chinese_ratio, 3)
                    }
                }
                json_line = json.dumps(obj, ensure_ascii=False)
                
                # å†™å…¥ä¸»æ–‡ä»¶
                out_f.write(json_line + '\n')
                valid_articles += 1
                
                # å†™å…¥æ ·ä¾‹æ–‡ä»¶
                if sample_count < sample_size:
                    sample_f.write(json_line + '\n')
                    sample_count += 1
                
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§æ–‡ç« æ•°é™åˆ¶
                if max_articles and valid_articles >= max_articles:
                    print(f"\nå·²è¾¾åˆ°æœ€å¤§æ–‡ç« æ•°é™åˆ¶: {max_articles}")
                    break
                
                break  # åªå¤„ç†æœ€æ–°ç‰ˆæœ¬
            
            # å¦‚æœè¾¾åˆ°æœ€å¤§æ–‡ç« æ•°é™åˆ¶ï¼Œé€€å‡ºå¤–å±‚å¾ªç¯
            if max_articles and valid_articles >= max_articles:
                break
            
            # å®šæœŸè¾“å‡ºè¿›åº¦
            if processed_pages % 1000 == 0:
                elapsed = time.time() - start_time
                pages_per_min = processed_pages / (elapsed / 60) if elapsed > 0 else 0
                print(f"å·²å¤„ç†é¡µé¢: {processed_pages:,}, æœ‰æ•ˆæ–‡ç« : {valid_articles:,}, "
                      f"æ ·ä¾‹: {sample_count}, é€Ÿåº¦: {pages_per_min:.0f}é¡µ/åˆ†é’Ÿ")
                out_f.flush()
                sample_f.flush()
    
    # è®¡ç®—æœ€ç»ˆç»Ÿè®¡æ•°æ®
    end_time = time.time()
    total_time = end_time - start_time
    
    print("\n" + "=" * 60)
    print("ğŸ“Š å¤„ç†å®Œæˆ - æ•ˆæœè¯„ä¼°æŠ¥å‘Š")
    print("=" * 60)
    
    # åŸºæœ¬ç»Ÿè®¡
    print(f"ğŸ“ˆ åŸºæœ¬ç»Ÿè®¡:")
    print(f"  æ€»é¡µé¢æ•°: {processed_pages:,}")
    print(f"  æœ‰æ•ˆæ–‡ç« : {valid_articles:,}")
    print(f"  æ ·ä¾‹æ•°é‡: {sample_count}")
    if max_articles:
        print(f"  æ–‡ç« æ•°é™åˆ¶: {max_articles:,} {'(å·²è¾¾åˆ°)' if valid_articles >= max_articles else '(æœªè¾¾åˆ°)'}")
    print(f"  è¿‡æ»¤æ¯”ä¾‹: {((processed_pages - valid_articles) / processed_pages * 100):.1f}%")
    print(f"  å¤„ç†æ—¶é—´: {total_time:.1f}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)")
    print(f"  å¤„ç†é€Ÿåº¦: {processed_pages / (total_time / 60):.0f}é¡µ/åˆ†é’Ÿ")
    
    if valid_articles > 0:
        # æ–‡æœ¬è´¨é‡æŒ‡æ ‡
        avg_length = sum(text_lengths) / len(text_lengths)
        avg_chinese_ratio = sum(chinese_ratios) / len(chinese_ratios)
        
        # é•¿åº¦åˆ†å¸ƒç»Ÿè®¡
        short_texts = sum(1 for l in text_lengths if l < 500)
        medium_texts = sum(1 for l in text_lengths if 500 <= l < 2000)
        long_texts = sum(1 for l in text_lengths if l >= 2000)
        
        # ä¸­æ–‡æ¯”ä¾‹åˆ†å¸ƒ
        high_chinese = sum(1 for r in chinese_ratios if r >= 0.8)
        medium_chinese = sum(1 for r in chinese_ratios if 0.5 <= r < 0.8)
        low_chinese = sum(1 for r in chinese_ratios if r < 0.5)
        
        print(f"\nğŸ“‹ è´¨é‡æŒ‡æ ‡:")
        print(f"  å¹³å‡æ–‡æœ¬é•¿åº¦: {avg_length:.0f}å­—ç¬¦")
        print(f"  å¹³å‡ä¸­æ–‡æ¯”ä¾‹: {avg_chinese_ratio:.1%}")
        print(f"  æ€»å­—ç¬¦æ•°: {total_text_length:,}")
        print(f"  æ€»ä¸­æ–‡å­—ç¬¦: {total_chinese_chars:,}")
        
        print(f"\nğŸ“Š é•¿åº¦åˆ†å¸ƒ:")
        print(f"  çŸ­æ–‡æœ¬ (<500å­—ç¬¦): {short_texts:,} ({short_texts/valid_articles:.1%})")
        print(f"  ä¸­ç­‰æ–‡æœ¬ (500-2000å­—ç¬¦): {medium_texts:,} ({medium_texts/valid_articles:.1%})")
        print(f"  é•¿æ–‡æœ¬ (>2000å­—ç¬¦): {long_texts:,} ({long_texts/valid_articles:.1%})")
        
        print(f"\nğŸˆ³ ä¸­æ–‡æ¯”ä¾‹åˆ†å¸ƒ:")
        print(f"  é«˜ä¸­æ–‡æ¯”ä¾‹ (â‰¥80%): {high_chinese:,} ({high_chinese/valid_articles:.1%})")
        print(f"  ä¸­ç­‰ä¸­æ–‡æ¯”ä¾‹ (50-80%): {medium_chinese:,} ({medium_chinese/valid_articles:.1%})")
        print(f"  ä½ä¸­æ–‡æ¯”ä¾‹ (<50%): {low_chinese:,} ({low_chinese/valid_articles:.1%})")
        

    print("=" * 60)

def main():
    parser = argparse.ArgumentParser(
        description="Wikipediaä¸­æ–‡æ•°æ®æ¸…æ´—å·¥å…· - ä¸“ä¸ºå¤§æ¨¡å‹é¢„è®­ç»ƒä¼˜åŒ–",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # å¤„ç†å®Œæ•´dumpæ–‡ä»¶
  python clean.py zhwiki-20250601-pages-articles-multistream1.xml-p1p187712.bz2
  
  # å¿«é€Ÿæµ‹è¯•ï¼šåªå¤„ç†1000ç¯‡æ–‡ç« 
  python clean.py dump.xml.bz2 --max-articles 1000
  
  # è‡ªå®šä¹‰æ‰€æœ‰å‚æ•°
  python clean.py dump.xml.bz2 --output clean.jsonl --sample sample.jsonl --sample-size 500 --max-articles 2000
        """
    )
    
    parser.add_argument(
        "dump_path",
        type=Path,
        help="ç»´åŸºç™¾ç§‘dumpæ–‡ä»¶è·¯å¾„ (*.xml.bz2)"
    )
    parser.add_argument(
        "--output",
        dest="output_path",
        type=Path,
        default="zhwiki_cleaned.jsonl",
        help="è¾“å‡ºJSONLæ–‡ä»¶è·¯å¾„ (é»˜è®¤: zhwiki_cleaned.jsonl)"
    )
    parser.add_argument(
        "--sample",
        dest="sample_path", 
        type=Path,
        default="sample_1000.jsonl",
        help="æ ·ä¾‹æ–‡ä»¶è·¯å¾„ (é»˜è®¤: sample_1000.jsonl)"
    )
    parser.add_argument(
        "--sample-size",
        dest="sample_size",
        type=int,
        default=1000,
        help="æ ·ä¾‹æ•°é‡ (é»˜è®¤: 1000)"
    )
    parser.add_argument(
        "--max-articles",
        dest="max_articles",
        type=int,
        help="æœ€å¤§å¤„ç†æ–‡ç« æ•°"
    )
    parser.add_argument(
        "--version",
        action="version",
        version="Wikipediaä¸­æ–‡æ¸…æ´—å·¥å…· v2.0"
    )
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not args.dump_path.exists():
        print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨: {args.dump_path}")
        sys.exit(1)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    args.output_path.parent.mkdir(parents=True, exist_ok=True)
    args.sample_path.parent.mkdir(parents=True, exist_ok=True)
    
    try:
        process_dump(
            str(args.dump_path),
            str(args.output_path), 
            str(args.sample_path),
            args.sample_size,
            args.max_articles
        )
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­å¤„ç†")
        sys.exit(1)
    except Exception as e:
        print(f"å¤„ç†å‡ºé”™: {e}")
        sys.exit(1)

if __name__ == '__main__':
    main()